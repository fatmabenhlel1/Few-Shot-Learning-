{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatmabenhlel1/Few-Shot-Learning-/blob/main/2D_cnn_early_fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4e51c144",
      "metadata": {
        "id": "4e51c144"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPool3D, Flatten, Dense, Dropout, Activation\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import GlobalAveragePooling3D, GlobalMaxPooling3D, Concatenate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pLbC0PKqTH3_",
      "metadata": {
        "id": "pLbC0PKqTH3_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78EMky-DS0Md",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78EMky-DS0Md",
        "outputId": "f99d1a8c-ca5d-40f3-e386-3558fa831c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dEVR9IwyhGvn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEVR9IwyhGvn",
        "outputId": "935f0d9d-b8cd-42a4-b834-92fea68fbe08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de dossiers : 7247\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_folders(directory):\n",
        "    return len([name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))])\n",
        "\n",
        "# Exemple d'utilisation\n",
        "\n",
        "directory3 = \"/content/drive/MyDrive/Database/data\"\n",
        "\n",
        "print(f\"Nombre de dossiers : {count_folders(directory3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "P8J3iitBjmF_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8J3iitBjmF_",
        "outputId": "e10f4d0c-4f7b-424e-85b6-9ff3b2c193b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier g√©n√©r√© avec 7247 vid√©os : all.txt\n"
          ]
        }
      ],
      "source": [
        "def extract_label(folder_name):\n",
        "    \"\"\"Extrait le label apr√®s le deuxi√®me underscore.\"\"\"\n",
        "    parts = folder_name.split(\"_\")\n",
        "    if len(parts) >= 3:\n",
        "        return int(parts[2])  # 0: Pt, 1: 72, 2: 1\n",
        "    else:\n",
        "        raise ValueError(f\"Impossible d'extraire le label depuis : {folder_name}\")\n",
        "\n",
        "\n",
        "def create_txt_list(data_dir, output_txt):\n",
        "    samples = []\n",
        "\n",
        "    for video_folder in os.listdir(data_dir):\n",
        "        label = extract_label(video_folder)\n",
        "        video_path = os.path.join(\"data\", video_folder)  # important : relatif √† \"data\"\n",
        "        samples.append(f\"{video_path} 1 {label}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    with open(output_txt, \"w\") as f:\n",
        "        f.writelines(samples)\n",
        "\n",
        "    print(f\"Fichier g√©n√©r√© avec {len(samples)} vid√©os : {output_txt}\")\n",
        "\n",
        "# Usage\n",
        "create_txt_list(\"/content/drive/MyDrive/Database/data\", \"all.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f9b18e74",
      "metadata": {
        "id": "f9b18e74"
      },
      "outputs": [],
      "source": [
        "def cnn2d_backbone():\n",
        "    inputs = Input(shape=(112, 112, 3))\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x_avg = GlobalAveragePooling2D()(x)\n",
        "    x_max = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    features = Concatenate()([x_avg, x_max])  # final vector per frame\n",
        "    model = Model(inputs, features)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "86fd7B8Vf5k8",
      "metadata": {
        "id": "86fd7B8Vf5k8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def video_embedding_model(input_dim=256, num_classes=4):\n",
        "    inputs = tf.keras.Input(shape=(input_dim,))\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "DAcxCBEEqznG",
      "metadata": {
        "id": "DAcxCBEEqznG"
      },
      "outputs": [],
      "source": [
        "def generator_train_batch(train_file, batch_size, num_classes, img_path, cnn_model):\n",
        "    with open(train_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    while True:\n",
        "        for i in range(0, len(lines), batch_size):\n",
        "            batch_lines = lines[i:i + batch_size]\n",
        "            X_batch = []\n",
        "            y_batch = []\n",
        "\n",
        "            for line in batch_lines:\n",
        "                try:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 3:\n",
        "                        print(f\"Malformed line skipped: {line.strip()}\")\n",
        "                        continue\n",
        "\n",
        "                    path = parts[0]\n",
        "                    label = int(parts[2])  # üü¢ Assuming the label is the third element\n",
        "\n",
        "                    folder_path = os.path.join(img_path, path)\n",
        "                    frame_files = sorted(os.listdir(folder_path))\n",
        "\n",
        "                    if len(frame_files) < 10:\n",
        "                        print(f\"Not enough frames in: {folder_path}\")\n",
        "                        continue\n",
        "\n",
        "                    frames = []\n",
        "                    for frame_file in frame_files[:10]:\n",
        "                        img_path_full = os.path.join(folder_path, frame_file)\n",
        "                        img = cv2.imread(img_path_full)\n",
        "\n",
        "                        if img is None:\n",
        "                            print(f\"Could not read image: {img_path_full}\")\n",
        "                            raise ValueError(\"Image is None\")\n",
        "\n",
        "                        img = cv2.resize(img, (112, 112))\n",
        "                        img = img / 255.0\n",
        "                        frames.append(img)\n",
        "\n",
        "                    frames = np.array(frames)  # shape (10, 112, 112, 3)\n",
        "\n",
        "                    # üü¢ Extract features from each frame\n",
        "                    features = cnn_model.predict(frames, verbose=0)  # (10, 256)\n",
        "                    video_embedding = np.mean(features, axis=0)      # (256,)\n",
        "                    X_batch.append(video_embedding)\n",
        "\n",
        "                    # üü¢ One-hot encode the label\n",
        "                    one_hot = tf.keras.utils.to_categorical(label, num_classes)\n",
        "                    y_batch.append(one_hot)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {line.strip()}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if len(X_batch) == 0:\n",
        "                continue\n",
        "\n",
        "            yield np.array(X_batch), np.array(y_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3f8oiPjAqydd",
      "metadata": {
        "id": "3f8oiPjAqydd"
      },
      "outputs": [],
      "source": [
        "def load_frames_from_folder(folder_path):\n",
        "    frames = []\n",
        "    for frame_name in sorted(os.listdir(folder_path)):\n",
        "        frame_path = os.path.join(folder_path, frame_name)\n",
        "        img = cv2.imread(frame_path)\n",
        "        img = cv2.resize(img, (112, 112))\n",
        "        frames.append(img)\n",
        "    frames = np.array(frames)  # (H, W, D, C) devient (H, D, W, C)\n",
        "    return frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "AN0PGqbluiyf",
      "metadata": {
        "id": "AN0PGqbluiyf"
      },
      "outputs": [],
      "source": [
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epoch_times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.start_time = datetime.datetime.now()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        duration = (datetime.datetime.now() - self.start_time).total_seconds()\n",
        "        self.epoch_times.append(duration)\n",
        "        print(f\" Temps pour l'epoch {epoch + 1}: {duration:.2f} secondes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b38cde47",
      "metadata": {
        "id": "b38cde47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "    # === Chemins ===\n",
        "    img_path = \"/content/drive/MyDrive/Database/\"  # correct path\n",
        "    train_file = \"all.txt\"  # pas /train_list.txt\n",
        "\n",
        "    # === Lire les fichiers pour compter les exemples ===\n",
        "    with open(train_file, \"r\") as f:\n",
        "        train_samples = len(f.readlines())\n",
        "\n",
        "    # === Param√®tres ===\n",
        "    num_classes = 4\n",
        "    batch_size = 8\n",
        "    epochs = 25\n",
        "\n",
        "    cnn_model = cnn2d_backbone()\n",
        "    model = video_embedding_model(input_dim=cnn_model.output_shape[1], num_classes=num_classes)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # === Cr√©ation dossier checkpoints si n√©cessaire ===\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    # === TensorBoard ===\n",
        "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "    # === Checkpoint Callback to save after each epoch ===\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=\"checkpoints/epoch_{epoch:02d}.h5\",\n",
        "        save_freq='epoch',\n",
        "        save_weights_only=False,\n",
        "        save_best_only=False,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # === Time Callback ===\n",
        "    time_callback = TimeHistory()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # === Entra√Ænement ===\n",
        "    history = model.fit(\n",
        "        generator_train_batch(train_file, batch_size, num_classes, img_path, cnn_model),\n",
        "        steps_per_epoch=122,\n",
        "        epochs=epochs,\n",
        "        callbacks=[tensorboard_callback, checkpoint_callback, time_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "    # === Sauvegarde finale ===\n",
        "    model.save(\"c3d_feature_extractor.h5\")\n",
        "    print(\"Feature extractor saved as c3d_feature_extractor.h5\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "giknclZCrkDl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "giknclZCrkDl",
        "outputId": "fc4d0012-0edc-4cc2-a8b0-d00e1857d436"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ],
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m65,664\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              ‚îÇ           \u001b[38;5;34m516\u001b[0m ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,180</span> (258.52 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,180\u001b[0m (258.52 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,180</span> (258.52 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,180\u001b[0m (258.52 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53s/step - accuracy: 0.8751 - loss: 0.7664 \n",
            "Epoch 1: saving model to checkpoints/epoch_01.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Temps pour l'epoch 1: 6356.91 secondes\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6357s\u001b[0m 53s/step - accuracy: 0.8741 - loss: 0.7685\n",
            "Epoch 2/25\n",
            "\u001b[1m  2/122\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m2:04:40\u001b[0m 62s/step - accuracy: 0.1250 - loss: 0.9774Not enough frames in: /content/drive/MyDrive/Database/data/Pt_12_0_seq41\n",
            "\u001b[1m  5/122\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:42:18\u001b[0m 52s/step - accuracy: 0.1311 - loss: 1.0830"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import traceback\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    GlobalAveragePooling2D,\n",
        "    GlobalMaxPooling2D,\n",
        "    Concatenate\n",
        ")\n",
        "\n",
        "# === Frame‚ÄêLevel CNN Backbone ===\n",
        "def cnn2d_backbone():\n",
        "    inputs = Input(shape=(112, 112, 3))\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x_avg = GlobalAveragePooling2D()(x)\n",
        "    x_max = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    features = Concatenate(name='frame_features')([x_avg, x_max])  # 512D features\n",
        "    return Model(inputs=inputs, outputs=features, name='cnn2d_backbone')\n",
        "\n",
        "\n",
        "# === Load model with correct architecture ===\n",
        "def load_backbone(model_path):\n",
        "    \"\"\"\n",
        "    Try to load a full model and extract the frame_features layer. If that fails,\n",
        "    rebuild the backbone and load weights by name.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        full_model = load_model(model_path, compile=False)\n",
        "        # Make sure our layer exists\n",
        "        if 'frame_features' not in {l.name for l in full_model.layers}:\n",
        "            raise ValueError(\"Layer 'frame_features' not found in loaded model.\")\n",
        "        return Model(\n",
        "            inputs=full_model.input,\n",
        "            outputs=full_model.get_layer('frame_features').output,\n",
        "            name='backbone_from_full_model'\n",
        "        )\n",
        "    except Exception:\n",
        "        print(f\"[load_backbone] Could not load full model from {model_path}.\")\n",
        "        traceback.print_exc()\n",
        "        print(\"[load_backbone] Falling back to rebuilding the backbone and loading weights by name...\")\n",
        "        backbone = cnn2d_backbone()\n",
        "        try:\n",
        "            backbone.load_weights(model_path, by_name=True)\n",
        "            print(\"[load_backbone] Weights loaded into backbone successfully.\")\n",
        "            return backbone\n",
        "        except Exception:\n",
        "            print(f\"[load_backbone] Failed to load weights from {model_path} into rebuilt backbone.\")\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "\n",
        "# === Load dataset ===\n",
        "def load_dataset(dataset_dir, input_shape=(112, 112), max_frames=10):\n",
        "    data = {}\n",
        "    for class_name in sorted(os.listdir(dataset_dir)):\n",
        "        class_path = os.path.join(dataset_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        clips = []\n",
        "        for clip_name in sorted(os.listdir(class_path)):\n",
        "            clip_path = os.path.join(class_path, clip_name)\n",
        "            frames = []\n",
        "            for frame_name in sorted(os.listdir(clip_path))[:max_frames]:\n",
        "                img = cv2.imread(os.path.join(clip_path, frame_name))\n",
        "                if img is None:\n",
        "                    continue\n",
        "                img = cv2.resize(img, input_shape)\n",
        "                frames.append(img.astype('float32') / 255.0)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                clips.append(np.stack(frames, axis=0))  # shape: (max_frames, H, W, C)\n",
        "\n",
        "        # need at least 2 clips per class for few‚Äêshot\n",
        "        if len(clips) >= 2:\n",
        "            data[class_name] = clips[:2]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# === Feature extraction ===\n",
        "def extract_clip_feature(model, clip_frames):\n",
        "    \"\"\"\n",
        "    clip_frames: np.array, shape (num_frames, H, W, C)\n",
        "    returns: single 512‚ÄêD embedding vector\n",
        "    \"\"\"\n",
        "    feats = model.predict(clip_frames, verbose=0)  # (num_frames, 512)\n",
        "    return feats.mean(axis=0)\n",
        "\n",
        "\n",
        "# === Few‚ÄêShot Episodes ===\n",
        "def run_episodes(data, model, num_episodes=1):\n",
        "    y_true, y_pred = [], []\n",
        "    class_names = list(data.keys())\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        support_feats, support_labels = [], []\n",
        "        query_feats, query_labels = [], []\n",
        "\n",
        "        # build support & query sets\n",
        "        for idx, cname in enumerate(class_names):\n",
        "            clips = data[cname]\n",
        "            random.shuffle(clips)\n",
        "            support, query = clips[:1][0], clips[1:2][0]\n",
        "\n",
        "            support_feats.append(extract_clip_feature(model, support))\n",
        "            support_labels.append(idx)\n",
        "\n",
        "            query_feats.append(extract_clip_feature(model, query))\n",
        "            query_labels.append(idx)\n",
        "\n",
        "        # nearest‚Äêneighbor classification\n",
        "        for qf in query_feats:\n",
        "            dists = [np.linalg.norm(qf - sf) for sf in support_feats]\n",
        "            y_pred.append(int(np.argmin(dists)))\n",
        "        y_true.extend(query_labels)\n",
        "\n",
        "        print(f\"Episode {ep}/{num_episodes} complete\")\n",
        "\n",
        "    return y_true, y_pred, class_names\n",
        "\n",
        "\n",
        "# === Main ===\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/content/checkpoints/epoch_01.h5\"\n",
        "    data_dir   = \"/content/drive/MyDrive/Database/clips_fsl_v2\"\n",
        "\n",
        "    backbone = load_backbone(model_path)\n",
        "    data     = load_dataset(data_dir)\n",
        "\n",
        "    y_true, y_pred, classes = run_episodes(data, backbone)\n",
        "    print(\"\\nFew-Shot Classification Report:\\n\")\n",
        "    print(classification_report(y_true, y_pred, target_names=classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoPSsp6wC1Z9",
        "outputId": "06c7367f-2b91-4b3a-9f96-2edd460dd79b"
      },
      "id": "VoPSsp6wC1Z9",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load_backbone] Could not load full model from /content/checkpoints/epoch_01.h5.\n",
            "[load_backbone] Falling back to rebuilding the backbone and loading weights by name...\n",
            "[load_backbone] Weights loaded into backbone successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-34-b0483e42e8b7>\", line 47, in load_backbone\n",
            "    raise ValueError(\"Layer 'frame_features' not found in loaded model.\")\n",
            "ValueError: Layer 'frame_features' not found in loaded model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1 complete\n",
            "\n",
            "Few-Shot Classification Report:\n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "          Carcinoma       1.00      1.00      1.00         1\n",
            "   Extreme_polipoid       1.00      1.00      1.00         1\n",
            "         Laryngitis       0.50      1.00      0.67         1\n",
            "Vocal_insufficiency       0.00      0.00      0.00         1\n",
            "        leukoplacia       1.00      1.00      1.00         1\n",
            "          papilloma       1.00      1.00      1.00         1\n",
            "               scar       1.00      1.00      1.00         1\n",
            "    vocal_fold_cyst       0.00      0.00      0.00         1\n",
            "\n",
            "           accuracy                           0.75         8\n",
            "          macro avg       0.69      0.75      0.71         8\n",
            "       weighted avg       0.69      0.75      0.71         8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}