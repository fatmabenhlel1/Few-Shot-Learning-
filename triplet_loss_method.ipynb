{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imageio import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehdi\\AppData\\Local\\Temp\\ipykernel_11460\\2695433577.py:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images = [imread(os.path.join(letter_path, f)) for f in os.listdir(letter_path)]\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_data():\n",
    "    data_path = r\"D:\\DATA\\archive\"\n",
    "    train_folder = os.path.join(data_path, 'images_background')\n",
    "    val_folder = os.path.join(data_path, 'images_evaluation')\n",
    "    save_path = 'SN_fsl\\SN_fsl2'\n",
    "\n",
    "    # Load or create dataset\n",
    "    def loadimgs(path):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Unzipping {os.path.basename(path)}\")\n",
    "            os.chdir(data_path)\n",
    "            os.system(f\"unzip {os.path.basename(path)}.zip\")\n",
    "            os.chdir(\"..\")\n",
    "            \n",
    "        X, y = [], []\n",
    "        for alphabet in os.listdir(path):\n",
    "            alphabet_path = os.path.join(path, alphabet)\n",
    "            for letter in os.listdir(alphabet_path):\n",
    "                letter_path = os.path.join(alphabet_path, letter)\n",
    "                images = [imread(os.path.join(letter_path, f)) for f in os.listdir(letter_path)]\n",
    "                X.append(np.array(images))\n",
    "        return np.array(X)\n",
    "\n",
    "    # Load and process data\n",
    "    X_train = loadimgs(train_folder)\n",
    "    X_val = loadimgs(val_folder)\n",
    "\n",
    "    # Preprocessing function\n",
    "    def preprocess(img):\n",
    "        img = 1.0 - img / 255.0  # Single image processing\n",
    "        img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "        return tf.image.resize(img, (28, 28)).numpy()  # Proper 3D input\n",
    "\n",
    "    # Process and augment with rotations\n",
    "    def process_and_augment(data):\n",
    "        processed = []\n",
    "        for char_class in data:\n",
    "            for img in char_class:\n",
    "                processed.append(preprocess(img))\n",
    "                # Add rotated versions\n",
    "                for k in range(1, 4):\n",
    "                    rotated = np.rot90(img, k)\n",
    "                    processed.append(preprocess(rotated))\n",
    "        return np.array(processed).reshape(-1, 20*4, 28, 28, 1)\n",
    "\n",
    "    X_train = process_and_augment(X_train)\n",
    "    X_val = process_and_augment(X_val)\n",
    "    \n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "X_train, X_val = load_and_process_data()\n",
    "\n",
    "# ----------------------------\n",
    "# Triplet Generation\n",
    "# ----------------------------\n",
    "class TripletGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.classes = dataset.shape[0]\n",
    "        self.samples_per_class = dataset.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataset) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch = []\n",
    "        for _ in range(self.batch_size):\n",
    "            # Random anchor class\n",
    "            anchor_class = np.random.randint(self.classes)\n",
    "            # Random positive sample\n",
    "            anchor_idx, positive_idx = np.random.choice(self.samples_per_class, 2, replace=False)\n",
    "            anchor = self.dataset[anchor_class, anchor_idx]\n",
    "            positive = self.dataset[anchor_class, positive_idx]\n",
    "            \n",
    "            # Random negative class\n",
    "            negative_class = np.random.randint(self.classes)\n",
    "            while negative_class == anchor_class:\n",
    "                negative_class = np.random.randint(self.classes)\n",
    "            negative = self.dataset[negative_class, np.random.randint(self.samples_per_class)]\n",
    "            \n",
    "            batch.append((anchor, positive, negative))\n",
    "        \n",
    "        anchors, positives, negatives = zip(*batch)\n",
    "        return [np.array(anchors), np.array(positives), np.array(negatives)], np.zeros(len(batch))\n",
    "\n",
    "# ----------------------------\n",
    "# Model Architecture\n",
    "# ----------------------------\n",
    "def create_base_network():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', \n",
    "                              input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
    "    ])\n",
    "\n",
    "base_network = create_base_network()\n",
    "\n",
    "# ----------------------------\n",
    "# Triplet Loss\n",
    "# ----------------------------\n",
    "def triplet_loss(margin=0.5):\n",
    "    def loss(_, y_pred):\n",
    "        anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
    "        \n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        \n",
    "        basic_loss = pos_dist - neg_dist + margin\n",
    "        return tf.reduce_mean(tf.maximum(basic_loss, 0.0))\n",
    "    return loss\n",
    "\n",
    "# ----------------------------\n",
    "# Training Setup\n",
    "# ----------------------------\n",
    "def create_siamese_model(base_network):\n",
    "    anchor_input = tf.keras.Input(shape=(28, 28, 1), name='anchor')\n",
    "    positive_input = tf.keras.Input(shape=(28, 28, 1), name='positive')\n",
    "    negative_input = tf.keras.Input(shape=(28, 28, 1), name='negative')\n",
    "    \n",
    "    anchor_embedding = base_network(anchor_input)\n",
    "    positive_embedding = base_network(positive_input)\n",
    "    negative_embedding = base_network(negative_input)\n",
    "    \n",
    "    merged_output = tf.stack([anchor_embedding, positive_embedding, negative_embedding], axis=1)\n",
    "    model = tf.keras.Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "siamese_model = create_siamese_model(base_network)\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=triplet_loss(0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 6s 27ms/step - loss: 0.4489 - val_loss: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.3180 - val_loss: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.3014 - val_loss: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.3021 - val_loss: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2820 - val_loss: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2846 - val_loss: 0.4998 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.2804 - val_loss: 0.4986 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2961 - val_loss: 0.4360 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2492 - val_loss: 0.4227 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2525 - val_loss: 0.2902 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2538 - val_loss: 0.2973 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2452 - val_loss: 0.4291 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2531 - val_loss: 0.3946 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2643 - val_loss: 0.3281 - lr: 1.0000e-06\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2584 - val_loss: 0.2741 - lr: 1.0000e-06\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2638 - val_loss: 0.2763 - lr: 1.0000e-06\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2402 - val_loss: 0.2642 - lr: 1.0000e-06\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2358 - val_loss: 0.2601 - lr: 1.0000e-06\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.2325 - val_loss: 0.2279 - lr: 1.0000e-06\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2298 - val_loss: 0.2606 - lr: 1.0000e-06\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2377 - val_loss: 0.2475 - lr: 1.0000e-06\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2228 - val_loss: 0.2366 - lr: 1.0000e-06\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2356 - val_loss: 0.2340 - lr: 1.0000e-07\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2424 - val_loss: 0.2494 - lr: 1.0000e-07\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2437 - val_loss: 0.2622 - lr: 1.0000e-07\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2348 - val_loss: 0.2420 - lr: 1.0000e-08\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2439 - val_loss: 0.2642 - lr: 1.0000e-08\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2325 - val_loss: 0.2988 - lr: 1.0000e-08\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.2253 - val_loss: 0.2573 - lr: 1.0000e-09\n"
     ]
    }
   ],
   "source": [
    "train_generator = TripletGenerator(X_train, batch_size=32)\n",
    "val_generator = TripletGenerator(X_val, batch_size=32)\n",
    "\n",
    "history = siamese_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=3),\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating k-shot performance...\n",
      "\n",
      "Final Results:\n",
      "k=1:\n",
      "  Accuracy:   0.6545\n",
      "  Precision:  0.6688\n",
      "  Recall:     0.6545\n",
      "  F1-Score:   0.6603\n",
      "\n",
      "k=2:\n",
      "  Accuracy:   0.6935\n",
      "  Precision:  0.7067\n",
      "  Recall:     0.6935\n",
      "  F1-Score:   0.6995\n",
      "\n",
      "k=3:\n",
      "  Accuracy:   0.7044\n",
      "  Precision:  0.7165\n",
      "  Recall:     0.7044\n",
      "  F1-Score:   0.7099\n",
      "\n",
      "k=4:\n",
      "  Accuracy:   0.7258\n",
      "  Precision:  0.7347\n",
      "  Recall:     0.7258\n",
      "  F1-Score:   0.7298\n",
      "\n",
      "k=5:\n",
      "  Accuracy:   0.7325\n",
      "  Precision:  0.7414\n",
      "  Recall:     0.7325\n",
      "  F1-Score:   0.7366\n",
      "\n",
      "k=6:\n",
      "  Accuracy:   0.7430\n",
      "  Precision:  0.7530\n",
      "  Recall:     0.7430\n",
      "  F1-Score:   0.7476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_k_shot(encoder, dataset, k_shot=5, n_way=3, test_episodes=1000):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for _ in range(test_episodes):\n",
    "        # Episode setup\n",
    "        classes = np.random.choice(len(dataset), n_way, replace=False)\n",
    "        support, query = [], []\n",
    "        true_labels = []\n",
    "        \n",
    "        # Sample selection\n",
    "        for i, cls in enumerate(classes):\n",
    "            samples = dataset[cls]\n",
    "            selected = np.random.choice(len(samples), k_shot + 15, replace=False)\n",
    "            support.extend(samples[selected[:k_shot]])\n",
    "            query.extend(samples[selected[k_shot:]])\n",
    "            true_labels.extend([i] * 15)  # 15 query samples per class\n",
    "            \n",
    "        # Embedding calculation\n",
    "        support_emb = encoder.predict(np.array(support), verbose=0)\n",
    "        query_emb = encoder.predict(np.array(query), verbose=0)\n",
    "        \n",
    "        # Prototype calculation\n",
    "        prototypes = [np.mean(support_emb[i*k_shot:(i+1)*k_shot], axis=0) \n",
    "                     for i in range(n_way)]\n",
    "        \n",
    "        # Prediction\n",
    "        preds = []\n",
    "        for q in query_emb:\n",
    "            distances = [np.linalg.norm(q - p) for p in prototypes]\n",
    "            preds.append(np.argmin(distances))\n",
    "            \n",
    "        # Convert to numpy arrays\n",
    "        true_labels = np.array(true_labels)\n",
    "        preds = np.array(preds)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = np.zeros(n_way)\n",
    "        fp = np.zeros(n_way)\n",
    "        fn = np.zeros(n_way)\n",
    "        \n",
    "        for cls in range(n_way):\n",
    "            tp[cls] = np.sum((preds == cls) & (true_labels == cls))\n",
    "            fp[cls] = np.sum((preds == cls) & (true_labels != cls))\n",
    "            fn[cls] = np.sum((true_labels == cls) & (preds != cls))\n",
    "            \n",
    "        # Avoid division by zero\n",
    "        precision = np.mean([tp[cls] / (tp[cls] + fp[cls]) if (tp[cls] + fp[cls]) > 0 else 0 \n",
    "                      for cls in range(n_way)])\n",
    "        recall = np.mean([tp[cls] / (tp[cls] + fn[cls]) if (tp[cls] + fn[cls]) > 0 else 0 \n",
    "                     for cls in range(n_way)])\n",
    "        f1 = np.mean([2 * (precision * recall) / (precision + recall) \n",
    "                    if (precision + recall) > 0 else 0 \n",
    "                    for cls in range(n_way)])\n",
    "        \n",
    "        # Store metrics\n",
    "        accuracies.append(np.mean(preds == true_labels))\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': np.mean(accuracies),\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'f1': np.mean(f1_scores)\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "print(\"\\nEvaluating k-shot performance...\")\n",
    "k_results = {}\n",
    "for k in range(1, 7):\n",
    "    metrics = evaluate_k_shot(base_network, X_val, k_shot=k)\n",
    "    k_results[k] = metrics\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFinal Results:\")\n",
    "for k, metrics in k_results.items():\n",
    "    print(f\"k={k}:\")\n",
    "    print(f\"  Accuracy:   {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision:  {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:     {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:   {metrics['f1']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
