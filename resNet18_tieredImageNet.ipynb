{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (from repo config file)\n",
    "DATA_ROOT = r\"F:\\datasets\\archive\\tiered_imagenet\"\n",
    "NUM_CLASSES = 351\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 84\n",
    "BASE_LR = 0.01\n",
    "LR_GAMMA = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 90\n",
    "LR_STEPS = [30, 60]\n",
    "NUM_WAYS = 5\n",
    "NUM_SHOTS = [1, 6]\n",
    "NUM_QUERIES = 15\n",
    "NUM_TASKS = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels,\n",
    "                         kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.functional.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self._initialize_weights()  # Initialize weights to improve convergence\n",
    "\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes) if num_classes else nn.Identity()\n",
    "\n",
    "    # Initialize weights to improve convergence\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        # Zero-initialize last BN in each residual branch\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, BasicBlock):\n",
    "                nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(nn.functional.relu(self.bn(x)))\n",
    "        return torch.cat([x, out], 1)\n",
    "# Data Transforms (official preprocessing)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(int(IMAGE_SIZE * 1.15)),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_dataloader(split):\n",
    "    return DataLoader(\n",
    "        ImageFolder(os.path.join(DATA_ROOT, split), \n",
    "                   transform=train_transform if split == \"train\" else test_transform),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=(split == \"train\"),\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "def train_model():\n",
    "    train_loader = get_dataloader(\"train\")\n",
    "    val_loader = get_dataloader(\"val\")\n",
    "    \n",
    "    # Change to ResNet18\n",
    "    model = ResNet18().to(DEVICE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=BASE_LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=LR_STEPS, gamma=LR_GAMMA)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #model output check\n",
    "    print(\"\\n--- Model Output Check ---\")\n",
    "    dummy_input = torch.randn(2, 3, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
    "    output = model(dummy_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Sample outputs:\", output[:2])\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"resnet18_tiered.pth\")  # Changed filename\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#evaluate fewshot\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_fewshot(model):\n",
    "    test_loader = get_dataloader(\"test\")\n",
    "    train_loader = get_dataloader(\"train\")\n",
    "\n",
    "    # Precompute mean feature from base classes\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_features = torch.cat([model(inputs.to(DEVICE)).cpu() \n",
    "                                  for inputs, _ in tqdm(train_loader, \n",
    "                                                        desc=\"Base Features\")])\n",
    "        mean_feature = train_features.mean(dim=0, keepdim=True)\n",
    "\n",
    "    def apply_transforms(features, transform):\n",
    "        if transform == \"L2N\":\n",
    "            return features / features.norm(p=2, dim=1, keepdim=True).clamp(min=1e-7)\n",
    "        elif transform == \"CL2N\":\n",
    "            centered = features - mean_feature\n",
    "            return centered / centered.norm(p=2, dim=1, keepdim=True).clamp(min=1e-7)\n",
    "        return features\n",
    "\n",
    "    results = {}\n",
    "    for num_shots in NUM_SHOTS:\n",
    "        print(f\"\\n{num_shots}-shot Classification Report:\")\n",
    "        \n",
    "        # Storage for all predictions and labels\n",
    "        all_preds = {t: [] for t in [\"UN\", \"L2N\", \"CL2N\"]}\n",
    "        all_labels = {t: [] for t in [\"UN\", \"L2N\", \"CL2N\"]}\n",
    "\n",
    "        for _ in tqdm(range(NUM_TASKS), desc=\"Tasks\"):\n",
    "            # Sample task\n",
    "            class_indices = np.random.choice(len(test_loader.dataset.classes), \n",
    "                                           NUM_WAYS, replace=False)\n",
    "            support, query = [], []\n",
    "            for c in class_indices:\n",
    "                samples = [i for i, (_, y) in enumerate(test_loader.dataset.samples) \n",
    "                          if y == c]\n",
    "                selected = np.random.choice(samples, num_shots + NUM_QUERIES, False)\n",
    "                support.extend(selected[:num_shots])\n",
    "                query.extend(selected[num_shots:])\n",
    "\n",
    "            # Process images\n",
    "            with torch.no_grad():\n",
    "                # Support features\n",
    "                sup_inputs = torch.stack([\n",
    "                    test_transform(Image.open(test_loader.dataset.samples[i][0]).convert(\"RGB\"))\n",
    "                    for i in support\n",
    "                ])\n",
    "                sup_features = model(sup_inputs.to(DEVICE)).cpu()\n",
    "                \n",
    "                # Query features\n",
    "                qry_inputs = torch.stack([\n",
    "                    test_transform(Image.open(test_loader.dataset.samples[i][0]).convert(\"RGB\"))\n",
    "                    for i in query\n",
    "                ])\n",
    "                qry_features = model(qry_inputs.to(DEVICE)).cpu()\n",
    "\n",
    "            # Generate labels once per task\n",
    "            labels = torch.arange(NUM_WAYS).repeat_interleave(NUM_QUERIES).numpy()\n",
    "\n",
    "            for transform in [\"UN\", \"L2N\", \"CL2N\"]:\n",
    "                # Transform features\n",
    "                t_sup = apply_transforms(sup_features, transform)\n",
    "                t_qry = apply_transforms(qry_features, transform)\n",
    "\n",
    "                # Calculate prototypes\n",
    "                prototypes = torch.stack([\n",
    "                    t_sup[i*num_shots:(i+1)*num_shots].mean(0) \n",
    "                    for i in range(NUM_WAYS)\n",
    "                ])\n",
    "\n",
    "                # Predictions\n",
    "                dists = torch.cdist(t_qry, prototypes)\n",
    "                preds = dists.argmin(dim=1).numpy()\n",
    "\n",
    "                # Store results\n",
    "                all_preds[transform].extend(preds)\n",
    "                all_labels[transform].extend(labels)\n",
    "\n",
    "        # Generate reports for each transform\n",
    "        for transform in [\"UN\", \"L2N\", \"CL2N\"]:\n",
    "            print(f\"\\nTransform: {transform}\")\n",
    "            print(classification_report(\n",
    "                all_labels[transform],\n",
    "                all_preds[transform],\n",
    "                target_names=[f\"Class {i}\" for i in range(NUM_WAYS)],\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            ))\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base Features: 100%|██████████| 7011/7011 [09:08<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-shot Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tasks: 100%|██████████| 10000/10000 [50:43<00:00,  3.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transform: UN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.4532    0.4567    0.4550    150000\n",
      "     Class 1     0.4607    0.4592    0.4599    150000\n",
      "     Class 2     0.4606    0.4554    0.4580    150000\n",
      "     Class 3     0.4546    0.4558    0.4552    150000\n",
      "     Class 4     0.4573    0.4591    0.4582    150000\n",
      "\n",
      "    accuracy                         0.4573    750000\n",
      "   macro avg     0.4573    0.4573    0.4573    750000\n",
      "weighted avg     0.4573    0.4573    0.4573    750000\n",
      "\n",
      "\n",
      "Transform: L2N\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.4775    0.4776    0.4775    150000\n",
      "     Class 1     0.4822    0.4816    0.4819    150000\n",
      "     Class 2     0.4821    0.4787    0.4804    150000\n",
      "     Class 3     0.4752    0.4785    0.4769    150000\n",
      "     Class 4     0.4797    0.4803    0.4800    150000\n",
      "\n",
      "    accuracy                         0.4793    750000\n",
      "   macro avg     0.4793    0.4793    0.4793    750000\n",
      "weighted avg     0.4793    0.4793    0.4793    750000\n",
      "\n",
      "\n",
      "Transform: CL2N\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.4800    0.4799    0.4799    150000\n",
      "     Class 1     0.4835    0.4845    0.4840    150000\n",
      "     Class 2     0.4851    0.4820    0.4835    150000\n",
      "     Class 3     0.4790    0.4803    0.4797    150000\n",
      "     Class 4     0.4816    0.4825    0.4821    150000\n",
      "\n",
      "    accuracy                         0.4818    750000\n",
      "   macro avg     0.4818    0.4818    0.4818    750000\n",
      "weighted avg     0.4818    0.4818    0.4818    750000\n",
      "\n",
      "\n",
      "6-shot Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tasks: 100%|██████████| 10000/10000 [55:32<00:00,  3.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transform: UN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.6405    0.6414    0.6409    150000\n",
      "     Class 1     0.6407    0.6375    0.6391    150000\n",
      "     Class 2     0.6417    0.6444    0.6431    150000\n",
      "     Class 3     0.6437    0.6450    0.6443    150000\n",
      "     Class 4     0.6410    0.6393    0.6401    150000\n",
      "\n",
      "    accuracy                         0.6415    750000\n",
      "   macro avg     0.6415    0.6415    0.6415    750000\n",
      "weighted avg     0.6415    0.6415    0.6415    750000\n",
      "\n",
      "\n",
      "Transform: L2N\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.6445    0.6445    0.6445    150000\n",
      "     Class 1     0.6431    0.6410    0.6420    150000\n",
      "     Class 2     0.6449    0.6483    0.6466    150000\n",
      "     Class 3     0.6473    0.6482    0.6477    150000\n",
      "     Class 4     0.6453    0.6430    0.6442    150000\n",
      "\n",
      "    accuracy                         0.6450    750000\n",
      "   macro avg     0.6450    0.6450    0.6450    750000\n",
      "weighted avg     0.6450    0.6450    0.6450    750000\n",
      "\n",
      "\n",
      "Transform: CL2N\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.6452    0.6455    0.6454    150000\n",
      "     Class 1     0.6440    0.6422    0.6431    150000\n",
      "     Class 2     0.6460    0.6490    0.6475    150000\n",
      "     Class 3     0.6489    0.6500    0.6494    150000\n",
      "     Class 4     0.6459    0.6433    0.6446    150000\n",
      "\n",
      "    accuracy                         0.6460    750000\n",
      "   macro avg     0.6460    0.6460    0.6460    750000\n",
      "weighted avg     0.6460    0.6460    0.6460    750000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #data pipeline check\n",
    "    \n",
    "    \n",
    "    # Train or load model\n",
    "    if not os.path.exists(\"resnet18_tiered.pth\"):  # Changed filename\n",
    "        print(\"Training ResNet-18...\")\n",
    "        model = train_model()\n",
    "    else:\n",
    "        print(\"Loading pretrained model...\")\n",
    "        model = ResNet18(num_classes=None).to(DEVICE)\n",
    "        state_dict = torch.load(\"resnet18_tiered.pth\", map_location=DEVICE)  # Changed filename\n",
    "        # Filter final FC layer weights\n",
    "        state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc\")}\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    # Remove final FC layer for feature extraction\n",
    "    model.fc = nn.Identity()\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate_fewshot(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
